{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/annarm1/compling/blob/main/tokenization_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f7aac47",
      "metadata": {
        "id": "7f7aac47"
      },
      "source": [
        "\n",
        "# Токенизация в NLP\n",
        "\n",
        "## Введение\n",
        "\n",
        "**Токенизация** – это процесс разбиения текста на минимальные единицы (токены): слова, подслова или символы.  \n",
        "Она является первым шагом во многих NLP задачах: анализ тональности, машинный перевод, чат-боты, обучение языковых моделей.\n",
        "\n",
        "В этом туториале мы разберём разные виды токенизации и библиотеки, которые их реализуют.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ff830a7",
      "metadata": {
        "id": "3ff830a7"
      },
      "source": [
        "\n",
        "## 1. Простые методы токенизации\n",
        "\n",
        "### 1.1. Разделение по пробелам\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c497c38f",
      "metadata": {
        "id": "c497c38f"
      },
      "outputs": [],
      "source": [
        "text = \"Hello, world! This is a test.\"\n",
        "text.split()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad40dfc0",
      "metadata": {
        "id": "ad40dfc0"
      },
      "source": [
        "\n",
        "### 1.2. Регулярные выражения\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11ad4956",
      "metadata": {
        "id": "11ad4956"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "text = \"Hello, world! This is a test.\"\n",
        "tokens = re.findall(r\"\\w+\", text)\n",
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c829e88",
      "metadata": {
        "id": "7c829e88"
      },
      "source": [
        "\n",
        "**Ограничения простых методов**:  \n",
        "- Не учитывают пунктуацию.  \n",
        "- Не работают с разными языками и морфологией.  \n",
        "- Не подходят для серьёзных задач.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e0bbff7",
      "metadata": {
        "id": "8e0bbff7"
      },
      "source": [
        "\n",
        "## 2. Токенизация с помощью библиотек\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50303eea",
      "metadata": {
        "id": "50303eea"
      },
      "source": [
        "\n",
        "### 2.1. NLTK  \n",
        "**Применение**: образовательные проекты, простая предобработка текста.  \n",
        "**Ограничения**: работает медленно на больших корпусах.  \n",
        "**Когда использовать**: для курсовых и лабораторных работ по основам NLP.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4ed988a",
      "metadata": {
        "id": "a4ed988a"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "text = \"Hello, world! This is a test.\"\n",
        "print(word_tokenize(text))\n",
        "print(sent_tokenize(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8854c741",
      "metadata": {
        "id": "8854c741"
      },
      "source": [
        "\n",
        "### 2.2. spaCy  \n",
        "**Применение**: промышленные проекты, морфология, синтаксис.  \n",
        "**Ограничения**: большие модели, требует ресурсов.  \n",
        "**Когда использовать**: для курсовых по синтаксису и морфологии, проектов по обработке естественного языка.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13d3373c",
      "metadata": {
        "id": "13d3373c"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Hello, world! This is a test.\")\n",
        "[t.text for t in doc]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b9f9ee4",
      "metadata": {
        "id": "0b9f9ee4"
      },
      "source": [
        "\n",
        "### 2.3. Stanza  \n",
        "**Применение**: многоязычные корпуса, морфология, синтаксис.  \n",
        "**Ограничения**: медленнее spaCy, требует загрузки моделей.  \n",
        "**Когда использовать**: в исследованиях и при работе с редкими языками.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8367d55c",
      "metadata": {
        "collapsed": true,
        "id": "8367d55c"
      },
      "outputs": [],
      "source": [
        "!pip install stanza\n",
        "import stanza\n",
        "\n",
        "stanza.download(\"en\")\n",
        "nlp = stanza.Pipeline(\"en\")\n",
        "doc = nlp(\"Hello, world! This is a test.\")\n",
        "[[word.text for word in sent.words] for sent in doc.sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e30a0fb",
      "metadata": {
        "id": "1e30a0fb"
      },
      "source": [
        "\n",
        "### 2.4. MosesTokenizer (sacremoses)  \n",
        "**Применение**: машинный перевод, предобработка текстов.  \n",
        "**Ограничения**: устаревающий стандарт.  \n",
        "**Когда использовать**: для экспериментов с классическими MT-системами.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8809846c",
      "metadata": {
        "id": "8809846c"
      },
      "outputs": [],
      "source": [
        "!pip install sacremoses\n",
        "\n",
        "from sacremoses import MosesTokenizer\n",
        "\n",
        "mt = MosesTokenizer()\n",
        "mt.tokenize(\"Hello, world! This is a test.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0670bd0",
      "metadata": {
        "id": "c0670bd0"
      },
      "source": [
        "\n",
        "### 2.5. gensim  \n",
        "**Применение**: подготовка данных для моделей `word2vec`, `fastText`.  \n",
        "**Ограничения**: базовая токенизация.  \n",
        "**Когда использовать**: при обучении собственных векторных моделей.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d521d38d",
      "metadata": {
        "id": "d521d38d"
      },
      "outputs": [],
      "source": [
        "#!pip install gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "simple_preprocess(\"Hello, world! This is a test.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a097e4e2",
      "metadata": {
        "id": "a097e4e2"
      },
      "source": [
        "\n",
        "### 2.6. HuggingFace Tokenizers  \n",
        "**Применение**: трансформеры, нейросетевые модели.  \n",
        "**Ограничения**: сложность, нужен GPU для обучения токенизаторов.  \n",
        "**Когда использовать**: в курсовых и проектах с BERT, GPT, T5.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8788801d",
      "metadata": {
        "id": "8788801d"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "tokens = tokenizer(\"Hello, world! This is a test.\")\n",
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b7ee318",
      "metadata": {
        "id": "4b7ee318"
      },
      "source": [
        "\n",
        "## 3. Посимвольная токенизация  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d24e768a",
      "metadata": {
        "id": "d24e768a"
      },
      "outputs": [],
      "source": [
        "list(\"Hello, world!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb08a046",
      "metadata": {
        "id": "bb08a046"
      },
      "source": [
        "\n",
        "## 4. Комбинированные методы  \n",
        "\n",
        "- **Послоговая токенизация** (актуально для языков с чёткой слоговой структурой, например, японский).  \n",
        "- **Гибридные методы**: совмещение слов и символов.  \n",
        "\n",
        "Пример: статья *Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models*.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "311105c2",
      "metadata": {
        "id": "311105c2"
      },
      "source": [
        "\n",
        "## 5. Сравнение токенизаторов  \n",
        "\n",
        "Попробуем применить разные методы к одному и тому же тексту.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "816fdf03",
      "metadata": {
        "id": "816fdf03"
      },
      "source": [
        "\n",
        "## 6. Практика\n",
        "\n",
        "Возьмём новостной текст, токенизируем разными способами и сравним.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d60a714e",
      "metadata": {
        "id": "d60a714e"
      },
      "outputs": [],
      "source": [
        "article = 'Natural language processing (NLP) is a field of artificial intelligence\\\n",
        "that gives computers the ability to understand text and spoken words in much the same way human beings can.'\n",
        "\n",
        "\"\"\"\n",
        "NLTK\n",
        "\"\"\"\n",
        "print(\"\\n------NLTK tokenization result-------\")\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "print(word_tokenize(article))\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "SpaCy\n",
        "\"\"\"\n",
        "print('\\n------SpaCy tokenization result-------')\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(article)\n",
        "print([t.text for t in doc])\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Gensim\n",
        "\"\"\"\n",
        "print('\\n------Gensim tokenization result-------')\n",
        "!pip install gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "print(simple_preprocess(article))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e575f14e",
      "metadata": {
        "id": "e575f14e"
      },
      "source": [
        "\n",
        "## 7. Задача для самостоятельного разбора  \n",
        "\n",
        "Прочитать статью:  \n",
        "**\"Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models\"** (https://arxiv.org/abs/1604.00788)\n",
        "\n",
        "### Напишите ответы на вопросы:\n",
        "1. Что значит Out-of-Vocabulary?  \n",
        "2. Как эту проблемы решили авторы статьи \"Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models\"?\n",
        "3. Какие еще типы токенизации мы разбирали на занятии?  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VpPnbGafikMc",
      "metadata": {
        "id": "VpPnbGafikMc"
      },
      "source": [
        "Ваш ответ здесь:\n",
        "\n",
        "1. Что значит Out-of-Vocabulary?  \n",
        "Это слова, которые отсутствуют в словаре модели (редкие слова), например потому что оно морфологически слишком сложное/новое или просто редко встречается. Их, как я поняла, называют unk-слова и видимо отмечают таким тегом.\n",
        "2. Как эту проблемы решили авторы статьи \"Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models\"?\n",
        "Авторы предлагают перевод слова на уровне символов с помощью character-level LSTM.\n",
        "3. Какие еще типы токенизации мы разбирали на занятии?  \n",
        "Мы разбирали токенизацию по словам, символам, предложениям, пробелам, по слогам (для японского), а также про токенизацию подсловную (когда слово разбивается на составные части, можно сказать почти морфемы)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "3ff830a7",
        "ad40dfc0",
        "50303eea",
        "8854c741",
        "0b9f9ee4",
        "1e30a0fb",
        "c0670bd0",
        "a097e4e2",
        "4b7ee318"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}